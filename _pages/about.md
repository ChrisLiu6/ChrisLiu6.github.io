---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}


# ü´† About Me
<span class='anchor' id='about-me'></span>
I am currently a second-year Ph.D. student at [MMLab](https://mmlab.ie.cuhk.edu.hk/), CUHK, supervised by Prof. [Hongsheng Li](https://www.ee.cuhk.edu.hk/~hsli/). Before that, I obtained my master's degree from [VIPL](https://vipl.ict.ac.cn/), where I was supervised by Prof. [Shiguang Shan](https://vipl.ict.ac.cn/people/sgshan/) and Prof. [Meina Kan](https://vipl.ict.ac.cn/people/mnkan/).

My current research focus is multimodal understanding and generation.

# ‚öôÔ∏è Projects
<span class='anchor' id='projects'></span>

<div class='paper-box-top'><div class='paper-box-image'><div><img src='images/projects/Z-Image.JPG' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Z-Image](https://github.com/Tongyi-MAI/Z-Image)

</div>
</div>

<div class='paper-box-bottom'><div class='paper-box-image'><div><img src='images/projects/accessory.JPG' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[LLaMA2-Accessory](https://github.com/Alpha-VLLM/LLaMA2-Accessory)

</div>
</div>

# üìù Publications
<span class='anchor' id='publications'></span>

<div class='paper-box-top'><div class='paper-box-image'><div><div class="badge">arXiv</div><img src='images/paper/decoupled-dmd.webp' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Decoupled DMD: CFG Augmentation as the Spear, Distribution Matching as the Shield](https://arxiv.org/abs/2511.22677)

**<span style="color:SkyBlue">Dongyang Liu</span>**, Peng Gao, David Liu, Ruoyi Du, Zhen Li, Qilong Wu, Xin Jin, Sihan Cao, Shifeng Zhang, Hongsheng Li, Steven Hoi

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv</div><img src='images/paper/lumina-video.JPG' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Lumina-Video: Efficient and Flexible Video Generation with Multi-scale Next-DiT](https://arxiv.org/abs/2502.06782)

**<span style="color:SkyBlue">Dongyang Liu</span>**, Shicheng Li, Yutong Liu, Zhen Li, Kai Wang, Xinyue Li, Qi Qin, Yufei Liu, Yi Xin, Zhong-Yu Li, Bin Fu, Chenyang Si, Yuewen Cao, Conghui He, Ziwei Liu, Yu Qiao, Qibin Hou, Hongsheng Li, Peng Gao

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arXiv</div><img src='images/paper/lumina-mgpt.JPG' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Lumina-mGPT: Illuminate flexible photorealistic text-to-image generation with multimodal generative pretraining](https://arxiv.org/abs/2408.02657)

**<span style="color:SkyBlue">Dongyang Liu\*</span>**, Shitian Zhao\*, Le Zhuo\*, Weifeng Lin\*, Yu Qiao, Hongsheng Li, Peng Gao

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/paper/lumina-next.JPG' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Lumina-Next: Making Lumina-T2X Stronger and Faster with Next-DiT](https://arxiv.org/abs/2406.18583)

Le Zhuo\*, Ruoyi Du\*, Han Xiao\*, Yangguang Li\*, **<span style="color:SkyBlue">Dongyang Li\*</span>**, Rongjie Huang\*, Wenze Liu\*, Lirui Zhao, Fu-Yun Wang, Zhanyu Ma, Xu Luo, Zehan Wang, Kaipeng Zhang, Xiangyang Zhu, Si Liu, Xiangyu Yue, Dingning Liu, Wanli Ouyang, Ziwei Liu, Yu Qiao, Hongsheng Li, Peng Gao

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2025</div><img src='images/paper/lumina-t2x.JPG' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers](https://arxiv.org/abs/2405.05945)

Peng Gao\*, Le Zhuo\*, **<span style="color:SkyBlue">Dongyang Liu\*</span>**, Ruoyi Du\*, Xu Luo\*, Longtian Qiu\*, Yuhang Zhang, Chen Lin, Rongjie Huang, Shijie Geng, Renrui Zhang, Junlin Xi, Wenqi Shao, Zhengkai Jiang, Tianshuo Yang, Weicai Ye, He Tong, Jingwen He, Yu Qiao, Hongsheng Li

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2024</div><img src='images/paper/sphinx-x.JPG' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models](https://arxiv.org/abs/2402.05935)

**<span style="color:SkyBlue">Dongyang Liu\*</span>**, Renrui Zhang\*, Longtian Qiu\*, Siyuan Huang\*, Weifeng Lin\*, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, Kaipeng Zhang, Wenqi Shao, Chao Xu, Conghui He, Junjun He, Hao Shao, Pan Lu, Hongsheng Li, Yu Qiao, Peng Gao

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/paper/sphinx.JPG' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models](https://arxiv.org/pdf/2311.07575)

Ziyi Lin\*, **<span style="color:SkyBlue">Chris Liu\*</span>**, Renrui Zhang\*, Peng Gao\*, Longtian Qiu\*, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, Yu Qiao

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2024</div><img src='images/paper/llama-adapter.JPG' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/abs/2303.16199)

Renrui Zhang\*, Jiaming Han\*, **<span style="color:SkyBlue">Chris Liu\*</span>**, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Yu Qiao, Peng Gao

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICLR 2024</div><img src='images/paper/metr.JPG' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[A Simple Romance Between Multi-Exit Vision Transformer and Token Reduction](https://openreview.net/forum?id=gJeYtRuguR)

**<span style="color:SkyBlue">Dongyang Liu</span>**, Meina Kan, Shiguang Shan, CHEN Xilin

</div>
</div>

<div class='paper-box-bottom'><div class='paper-box-image'><div><div class="badge">ICLR 2023</div><img src='images/paper/fcfd.JPG' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Function-Consistent Feature Distillation](https://arxiv.org/abs/2304.11832)

**<span style="color:SkyBlue">Dongyang Liu</span>**, Meina Kan, Shiguang Shan, CHEN Xilin

</div>
</div>

# üìñ Educations
<span class='anchor' id='educations'></span>
- *2024.09 - now*: MMLab, The Chinese University of Hongkong (Ph.D.)
- *2021.09 - 2024.06*: VIPL Lab, Institute of Computing Technology, Chinese Academy of Sciences (Master)
- *2017.09 - 2021.06*: School of Software Engineering, Tongji University. (Bachelor)
